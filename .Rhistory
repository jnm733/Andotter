y = NA
# Try to catch that error (NA) we just created
catch_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
# check result if error exists, otherwise the function works fine.
return(y)
}
#Función para limpiar el tweet
cleanTweets = function(tweet){
#  remove html links, which are not required for sentiment analysis
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
# First we will remove retweet entities from the stored tweets (text)
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
# Then remove all "#Hashtag"
tweet = gsub("#\\w+", " ", tweet)
# Then remove all "@people"
tweet = gsub("@\\w+", " ", tweet)
# Then remove all the punctuation
#tweet = gsub("[[:punct:]]", " ", tweet)
# Then remove numbers, we need only text for analytics
tweet = gsub("[[:digit:]]", " ", tweet)
tweet = gsub("Ã¡", "a", tweet)
tweet = gsub("Ã©", "e", tweet)
tweet = gsub("Ã³", "o", tweet)
tweet = gsub("Ã±", "ñ", tweet)
tweet = gsub("[[:punct:]]", " ", tweet)
tweet = gsub("ã", "i", tweet)
tweet = gsub("Ã ", "i", tweet)
# finally, we remove unnecessary spaces (white spaces, tabs etc)
tweet = gsub("[ \t]{2,}", " ", tweet)
tweet = gsub("^\\s+|\\s+$", "", tweet)
tweet = chartr('áéíóú','aeiou', tweet)
# if anything else, you feel, should be removed, you can. For example "slang words" etc using the above function and methods.
# Next we'll convert all the word in lower case. This makes uniform pattern.
tweet = catch.error(tweet)
tweet
}
cleanTweetsAndRemoveNAs = function(Tweets) {
TweetsCleaned = sapply(Tweets, cleanTweets)
# Remove the "NA" tweets from this tweet list
TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
names(TweetsCleaned) = NULL
# Remove the repetitive tweets from this tweet list
TweetsCleaned = unique(TweetsCleaned)
TweetsCleaned
}
TextTweetsSpanish <- sapply(AnalisisTweetsSpanish, function(x) x$getText())
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
AnalisisTweetsLimpioSpanish = cleanTweetsAndRemoveNAs(TextTweetsSpanish)
AnalisisTweetsLimpioSpanish
library(stringr)
#Función para convertir en minuscula
catch.error = function(x)
{
# let us create a missing value for test purpose
y = NA
# Try to catch that error (NA) we just created
catch_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
# check result if error exists, otherwise the function works fine.
return(y)
}
#Función para limpiar el tweet
cleanTweets = function(tweet){
#  remove html links, which are not required for sentiment analysis
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
# First we will remove retweet entities from the stored tweets (text)
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
# Then remove all "#Hashtag"
tweet = gsub("#\\w+", " ", tweet)
# Then remove all "@people"
tweet = gsub("@\\w+", " ", tweet)
# Then remove all the punctuation
#tweet = gsub("[[:punct:]]", " ", tweet)
# Then remove numbers, we need only text for analytics
tweet = gsub("[[:digit:]]", " ", tweet)
tweet = gsub("â", "¿", tweet)
tweet = gsub("Ã¡", "a", tweet)
tweet = gsub("Ã©", "e", tweet)
tweet = gsub("Ã³", "o", tweet)
tweet = gsub("Ã±", "ñ", tweet)
tweet = gsub("[[:punct:]]", " ", tweet)
tweet = gsub("ã", "i", tweet)
tweet = gsub("Ã ", "i", tweet)
# finally, we remove unnecessary spaces (white spaces, tabs etc)
tweet = gsub("[ \t]{2,}", " ", tweet)
tweet = gsub("^\\s+|\\s+$", "", tweet)
tweet = chartr('áéíóú','aeiou', tweet)
# if anything else, you feel, should be removed, you can. For example "slang words" etc using the above function and methods.
# Next we'll convert all the word in lower case. This makes uniform pattern.
tweet = catch.error(tweet)
tweet
}
cleanTweetsAndRemoveNAs = function(Tweets) {
TweetsCleaned = sapply(Tweets, cleanTweets)
# Remove the "NA" tweets from this tweet list
TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
names(TweetsCleaned) = NULL
# Remove the repetitive tweets from this tweet list
TweetsCleaned = unique(TweetsCleaned)
TweetsCleaned
}
TextTweetsSpanish <- sapply(AnalisisTweetsSpanish, function(x) x$getText())
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
AnalisisTweetsLimpioSpanish = cleanTweetsAndRemoveNAs(TextTweetsSpanish)
AnalisisTweetsLimpioSpanish
library(stringr)
#Función para convertir en minuscula
catch.error = function(x)
{
# let us create a missing value for test purpose
y = NA
# Try to catch that error (NA) we just created
catch_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
# check result if error exists, otherwise the function works fine.
return(y)
}
#Función para limpiar el tweet
cleanTweets = function(tweet){
#  remove html links, which are not required for sentiment analysis
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
# First we will remove retweet entities from the stored tweets (text)
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
# Then remove all "#Hashtag"
tweet = gsub("#\\w+", " ", tweet)
# Then remove all "@people"
tweet = gsub("@\\w+", " ", tweet)
# Then remove all the punctuation
#tweet = gsub("[[:punct:]]", " ", tweet)
# Then remove numbers, we need only text for analytics
tweet = gsub("[[:digit:]]", " ", tweet)
tweet = gsub("â", "¿", tweet)
tweet = gsub("Ã¡", "a", tweet)
tweet = gsub("Ã©", "e", tweet)
tweet = gsub("Ã³", "o", tweet)
tweet = gsub("Ã±", "ñ", tweet)
tweet = gsub("[[:punct:]]", " ", tweet)
tweet = gsub("ã", "i", tweet)
tweet = gsub("Ã ", "i", tweet)
tweet = gsub("\n", " ", tweet)
# finally, we remove unnecessary spaces (white spaces, tabs etc)
tweet = gsub("[ \t]{2,}", " ", tweet)
tweet = gsub("^\\s+|\\s+$", "", tweet)
tweet = chartr('áéíóú','aeiou', tweet)
# if anything else, you feel, should be removed, you can. For example "slang words" etc using the above function and methods.
# Next we'll convert all the word in lower case. This makes uniform pattern.
tweet = catch.error(tweet)
tweet
}
cleanTweetsAndRemoveNAs = function(Tweets) {
TweetsCleaned = sapply(Tweets, cleanTweets)
# Remove the "NA" tweets from this tweet list
TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
names(TweetsCleaned) = NULL
# Remove the repetitive tweets from this tweet list
TweetsCleaned = unique(TweetsCleaned)
TweetsCleaned
}
TextTweetsSpanish <- sapply(AnalisisTweetsSpanish, function(x) x$getText())
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
AnalisisTweetsLimpioSpanish = cleanTweetsAndRemoveNAs(TextTweetsSpanish)
AnalisisTweetsLimpioSpanish
TextTweetsSpanish <- sapply(AnalisisTweetsSpanish, function(x) x$getText())
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
AnalisisTweetsLimpioSpanish = cleanTweetsAndRemoveNAs(TextTweetsSpanish)
AnalisisTweetsLimpioSpanish
TextTweetsSpanish <- sapply(AnalisisTweetsSpanish, function(x) x$getText())
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
TextTweetsSpanish
runApp()
runApp()
tendencia = "Monfils"
AnalisisTweetsSpanish = searchTwitter(tendencia, n, lang = lang, resultType = "recent")
AnalisisTweetsSpanish
#AnalisisTweetsSpanish = strip_retweets(AnalisisTweetsSpanish, strip_manual = TRUE, strip_mt = TRUE)
TextTweetsSpanish <- sapply(AnalisisTweetsSpanish, function(x) x$getText())
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
TextTweetsSpanish
AnalisisTweetsLimpioSpanish = cleanTweetsAndRemoveNAs(TextTweetsSpanish)
AnalisisTweetsLimpioSpanish
TextTweetsSpanish <- sapply(AnalisisTweetsSpanish, function(x) x$getText())
TextTweetsSpanish
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
TextTweetsSpanish
tweet = TextTweetsSpanish[1]
tweet
tweet = TextTweetsSpanish[100]
tweet
tweet = TextTweetsSpanish[99]
tweet
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
# Then remove all "#Hashtag"
tweet = gsub("#\\w+", " ", tweet)
# Then remove all "@people"
tweet = gsub("@\\w+", " ", tweet)
# Then remove all the punctuation
#tweet = gsub("[[:punct:]]", " ", tweet)
# Then remove numbers, we need only text for analytics
tweet = gsub("[[:digit:]]", " ", tweet)
tweet
tweet = TextTweetsSpanish[99]
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
# First we will remove retweet entities from the stored tweets (text)
# Then remove all "#Hashtag"
tweet = gsub("#\\w+", " ", tweet)
# Then remove all "@people"
tweet = gsub("@\\w+", " ", tweet)
tweet
tweet = gsub("â", "¿", tweet)
tweet = gsub("Ã¡", "a", tweet)
tweet = gsub("Ã©", "e", tweet)
tweet = gsub("Ã³", "o", tweet)
tweet = gsub("Ã±", "ñ", tweet)
tweet = gsub("[[:punct:]]", " ", tweet)
tweet
tweet = gsub("ã", "i", tweet)
tweet = gsub("Ã ", "i", tweet)
tweet = gsub("[[:digit:]]", " ", tweet)
tweet = gsub("\n", " ", tweet)
tweet
library(stringr)
#Función para convertir en minuscula
catch.error = function(x)
{
# let us create a missing value for test purpose
y = NA
# Try to catch that error (NA) we just created
catch_error = tryCatch(tolower(x), error=function(e) e)
# if not an error
if (!inherits(catch_error, "error"))
y = tolower(x)
# check result if error exists, otherwise the function works fine.
return(y)
}
#Función para limpiar el tweet
cleanTweets = function(tweet){
#  remove html links, which are not required for sentiment analysis
tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)
# First we will remove retweet entities from the stored tweets (text)
tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)
# Then remove all "#Hashtag"
tweet = gsub("#\\w+", " ", tweet)
# Then remove all "@people"
tweet = gsub("@\\w+", " ", tweet)
# Then remove all the punctuation
#tweet = gsub("[[:punct:]]", " ", tweet)
# Then remove numbers, we need only text for analytics
tweet = gsub("â", "¿", tweet)
tweet = gsub("Ã¡", "a", tweet)
tweet = gsub("Ã©", "e", tweet)
tweet = gsub("Ã³", "o", tweet)
tweet = gsub("Ã±", "ñ", tweet)
tweet = gsub("[[:punct:]]", " ", tweet)
tweet = gsub("ã", "i", tweet)
tweet = gsub("Ã ", "i", tweet)
tweet = gsub("[[:digit:]]", " ", tweet)
tweet = gsub("\n", " ", tweet)
# finally, we remove unnecessary spaces (white spaces, tabs etc)
tweet = gsub("[ \t]{2,}", " ", tweet)
tweet = gsub("^\\s+|\\s+$", "", tweet)
tweet = chartr('áéíóú','aeiou', tweet)
# if anything else, you feel, should be removed, you can. For example "slang words" etc using the above function and methods.
# Next we'll convert all the word in lower case. This makes uniform pattern.
tweet = catch.error(tweet)
tweet
}
cleanTweetsAndRemoveNAs = function(Tweets) {
TweetsCleaned = sapply(Tweets, cleanTweets)
# Remove the "NA" tweets from this tweet list
TweetsCleaned = TweetsCleaned[!is.na(TweetsCleaned)]
names(TweetsCleaned) = NULL
# Remove the repetitive tweets from this tweet list
TweetsCleaned = unique(TweetsCleaned)
TweetsCleaned
}
TextTweetsSpanish <- sapply(AnalisisTweetsSpanish, function(x) x$getText())
TextTweetsSpanish = iconv(TextTweetsSpanish, "latin1", "UTF-8", sub="")
TextTweetsSpanish
AnalisisTweetsLimpioSpanish = cleanTweetsAndRemoveNAs(TextTweetsSpanish)
AnalisisTweetsLimpioSpanish
runApp()
runGitHub("Andotter", "jnm733")
library(shiny)
runGitHub("Andotter", "jnm733")
shiny::runApp()
shiny::runApp()
library(twitteR)
limit = getCurRateLimitInfo()
#Carga de librerias
#install.packages(c("devtools", "rjson", "bit64", "httr"))
library(devtools)
#install.packages("twitteR")
library(twitteR)
#Credenciales Twitter
api_key<- "PyRGBHE4ZLk65LqQXMwhevAP1"
api_secret<- "dxZoDlqNVGv59jgsApFfSCPeFOllf91KgoFiIGylcwl6V65gSG"
access_token<- "514332205-NASGrkpioaIcz8QeqQz9Z1r6ni3sud6YhzUTCDq5"
access_token_secret<- "xSyAbnINaC1aGOWoQ5NBn4Ncxa9T79VlR4wz2anFWp9A7"
#Conexión OAuth
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
limit = getCurRateLimitInfo()
limit
View(limit)
shiny::runApp()
View(limit)
runApp()
#Carga de librerias
#install.packages(c("devtools", "rjson", "bit64", "httr"))
library(devtools)
#install.packages("twitteR")
library(twitteR)
#Credenciales Twitter
api_key<- "PyRGBHE4ZLk65LqQXMwhevAP1"
api_secret<- "dxZoDlqNVGv59jgsApFfSCPeFOllf91KgoFiIGylcwl6V65gSG"
access_token<- "514332205-NASGrkpioaIcz8QeqQz9Z1r6ni3sud6YhzUTCDq5"
access_token_secret<- "xSyAbnINaC1aGOWoQ5NBn4Ncxa9T79VlR4wz2anFWp9A7"
#Conexión OAuth
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
limit = getCurRateLimitInfo()
View(limit)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
paises = read.csv("Data/paises.csv")
colnames(paises) <- c("iso3166", "latitude", "longitude", "country")
View(paises)
paisesList = paises$country
View(paises)
runApp()
runApp()
paises = read.csv("Data/iso_3166_2_countries.csv")
View(paises)
runApp()
runApp()
runApp()
paises = read.csv("Data/iso_3166_2_countries.csv")
paisesList = paises$Common.Name
paisesList = subset(paises, Common.Name == "Spain")
View(paisesList)
paises = read.csv("Data/iso_3166_2_countries.csv")
paises = subset(paises, Common.Name == "Spain")
paisesList = paises$Common.Name
runApp()
paises = read.csv("Data/iso_3166_2_countries.csv")
runApp()
List = unique(as.list(Locs$country))
write.csv(List, file = "Data/paisesList.csv")
dfPaises = data.frame("Pais" = List)
View(dfPaises)
dfPaises = ldply(tweets, function(List) List$toDataFrame())
View(dfPaises)
dfPaises = ldply(List, function(List) List$toDataFrame())
dfPaises = as.data.frame.list(List)
View(dfPaises)
List = unique(as.list(Locs$country))
paises = read.csv("Data/iso_3166_2_countries.csv")
paisesList = paises$Capital
paises = read.csv("Data/paises.csv")
View(paises)
paisesList = unique(as.list(paises$nombre))
runApp()
paisesList
paisesList = paises$nombre
paisesList
paisesList[1]
paises = read.csv("Data/Countries-Europe.csv")
View(paises)
paisesList = paises$name
runApp()
runApp()
runApp()
runApp()
List = unique(as.list(Locs$country))
runApp()
runApp()
runApp()
paises = read.csv("Data/paises.csv")
colnames(paises) <- c("iso3166", "latitude", "longitude", "country")
paises = subset(paises, country == countryInput)
countryInput = "Spain"
paises = subset(paises, country == countryInput)
View(paises)
geocode = paste("", paises$latitude[1], sep = ", ")
geocode = paste(paises$latitude[1], ", ")
geocode = paste("", paises$latitude[1])
geocode = paste(paises$latitude[1], ", ")
geocode = paste(geocode, paises$longitude[1], sep=", ")
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=", ")
geocode = paste(geocode, paises$latitude[1], sep=", ")
geocode = paste(geocode, "506000km", sep=", ")
tendencia = "iphone"
n = 50
lang = "es"
AnalisisTweetsSpanish = searchTwitter(tendencia, n, lang = lang, resultType = "recent", geocode = geocode)
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=", ")
geocode = paste(geocode, paises$latitude[1], sep=", ")
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=", ")
geocode = paste(geocode, "506000km", sep=", ")
AnalisisTweetsSpanish = searchTwitter(tendencia, n, lang = lang, resultType = "recent", geocode = geocode)
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=", ")
geocode = paste(geocode, "506000km", sep=", ")
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=',')
lang = "en"
geocode="57.3906,-46.8855, 35mi"
AnalisisTweetsSpanish = searchTwitter(tendencia, n, lang = lang, resultType = "recent", geocode = geocode)
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=',')
geocode = paste(geocode, "506000km", sep=", ")
geocode="40.4167754,-3.7037901999999576, 506000km"
AnalisisTweetsSpanish = searchTwitter(tendencia, n, lang = lang, resultType = "recent", geocode = geocode)
lang = "es"
geocode="40.4167754,-3.7037901999999576, 700km"
AnalisisTweetsSpanish = searchTwitter(tendencia, n, lang = lang, resultType = "recent", geocode = geocode)
AnalisisTweetsSpanish
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=',')
geocode = paste(geocode, "506000km", sep=", ")
geocode="40.4167754,-3.7037901999999576, 700km"
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=',')
geocode = paste(geocode, "700km", sep=", ")
AnalisisTweetsSpanish = searchTwitter(tendencia, n, lang = lang, resultType = "recent", geocode = geocode)
AnalisisTweetsSpanish
paises = read.csv("Data/paises.csv")
colnames(paises) <- c("iso3166", "latitude", "longitude", "country")
paises = subset(paises, country == countryInput)
geocode = paises$latitude[1]
geocode = paste(geocode, paises$longitude[1], sep=',')
geocode = paste(geocode, "700km", sep=", ")
AnalisisTweetsSpanish = searchTwitter(tendencia, n, lang = lang, resultType = "recent", geocode = geocode)
runApp()
#Carga de librerias
#install.packages(c("devtools", "rjson", "bit64", "httr"))
library(devtools)
#install.packages("twitteR")
library(twitteR)
#Credenciales Twitter
api_key<- "PyRGBHE4ZLk65LqQXMwhevAP1"
api_secret<- "dxZoDlqNVGv59jgsApFfSCPeFOllf91KgoFiIGylcwl6V65gSG"
access_token<- "514332205-NASGrkpioaIcz8QeqQz9Z1r6ni3sud6YhzUTCDq5"
access_token_secret<- "xSyAbnINaC1aGOWoQ5NBn4Ncxa9T79VlR4wz2anFWp9A7"
#Conexión OAuth
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
runApp()
runApp()
runApp()
runApp()
}
runApp()
runApp()
runApp()
runApp()
#Carga de librerias
#install.packages(c("devtools", "rjson", "bit64", "httr"))
library(devtools)
#install.packages("twitteR")
library(twitteR)
#Credenciales Twitter
api_key<- "PyRGBHE4ZLk65LqQXMwhevAP1"
api_secret<- "dxZoDlqNVGv59jgsApFfSCPeFOllf91KgoFiIGylcwl6V65gSG"
access_token<- "514332205-NASGrkpioaIcz8QeqQz9Z1r6ni3sud6YhzUTCDq5"
access_token_secret<- "xSyAbnINaC1aGOWoQ5NBn4Ncxa9T79VlR4wz2anFWp9A7"
#Conexión OAuth
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
shiny::runApp()
library(rsconnect)
deployApp()
